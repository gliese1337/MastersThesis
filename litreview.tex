\chapter{Review of Literature}

Prior to almost any other natural language processing task, the first task that must be done with any text is tokenization (also called segmentation, lexing, or word breaking)- figuring out where the words (or other logical units) are. 

This problem is most obvious in the context of automatic speech recognition (ASR), where it is further necessary to convert continuous phonetic data into discrete phonemes. The standard solution in this context is to use Hidden Markov Models (HMMs) encoding the possible words of the language to transduce phonetic data into sequences of potentially-overlapping \textit{possible} phonemes and words\cite{varile97}. The correct tokenization may then be selected either by applying the Viterbi algorithm over a pre-generated lattice of "word hypotheses"\cite{aubert94}, or by performing an on-line beam search over possible tokenizations as new possibilities are generated\cite{paul94}.

If we ignore the the issue of transducing phonetic data into discrete phonemes, however, the problem of identifying logical groups in a stream of discrete lower-level symbols still remains, and we can isolate this aspect of the problem by working with written text, assembling morphemes and words from a stream of graphemes. I general, tokenization of written text\footnote{In corpus studies, "text" is often used to refer to any reification of language, be it spoken or written; from here on out however, I will assume that "text" refers only to written communication.} may be a harder problem than tokenization of speech since writing generally does not reflect spoken language losslessly, and fewer boundary clues (such as prosodic information) will be available. Nevertheless, the fact that reading is possible is proof that text tokenization can be done.

\section{Tokenization of English}
In English and many other languages, the problem of text tokenization is often considered trivial: simply split words on whitespace, which substitutes for many of the prosodic clues available in speech. Aken\cite{aken11}, for example, studied the problem of tokenizing continuous English text with spaces removed, but only as a proxy for understanding human tokenization strategies more generally which allowed eliminating the irrelevant complication of transducing phonemes, not with an eye towards any practical application for the study of English. While often good enough, however, the space-splitting approach is far from perfectly accurate. Even with perfectly clean data, this approach fails to account for:
\begin{enumerate}
	\item Word boundaries that correctly occur in the absence of whitespace (e.g., before punctuation, in specialized genres like URLs, or simply due to typographical errors)
	\item Whitespace that may not indicate a relevant boundary in certain applications (e.g., spaces in compounds, idioms, and borrowed foreign-language phrases used as single logical lexical units).
\end{enumerate}
These complications can be accounted for in various ways, but this must be done on an individual language-by-language basis. Even languages that all use the Roman alphabet, there are significant differences in whitespace and punctuation conventions. The tokenization conventions for the Penn Treebank, for example, separate "most punctuation" \cite{treebank}, but depend on prior knowledge of sentence boundaries to help disambiguate final periods from periods indicating abbreviations and on English-specific knowledge about the structure of contractions and cliticized forms.

Aken's system relies purely on statistical information available within a single text to infer where the words in that text are segmented; in particular, it keeps track of recurring sequences, identifying sequences that recur with greater than a certain threshold frequency as possible word types, and further keeps track of the transition probabilities between known sequences to determine the optimal path through a lattice of tokenization hypotheses\cite{aken11}. This algorithm was tested against both randomly generated strings of words from a dictionary, and a small sample of natural English text, which revealed a weakness in identifying boundaries in sequences of multiple short words, such as "in a"; this is a particular weakness which is shared by other statistical algorithms as well. This approach taken in isolation would also be expected to perform poorly on a text containing a large number of hapax legomena, which would pass the frequency threshold for recognition, making it potentially less suitable for languages with extensive productive morphology.

Aken's algorithm also depends on an implicit assumption about what the definition of a "word" is: a fixed sequence of symbols that can recur multiple times. While this assumption may indeed be a useful one for vocabulary induction from unlabeled data, it is not necessarily the best definition of "word" for all circumstances, especially across multiple languages. Additionally, there is ambiguity in the usage of "word" to refer to a particular sequence of symbols vs. a particular instance of that sequence in a text. Unfortunately, per Islam\cite{islam07}, there are in fact no widely accepted definitions for what constitutes a "word", even within a single language, let alone cross-linguistically. Different native speakers often segment text in different ways, and the rate of agreement between human judges can be less than 80\%, making it impossible to construct a single "gold standard" to evaluate results between systems that employ different conventions, whether implicit or explicit.
For these reasons, it is often useful to dispense with the problem of defining a "word" at all and, when precision is required, to instead refer to \textit{types}, meaning specific abstract sequences of symbols, and \textit{tokens}, meaning specific instances of a type at specific locations in a text. Tokenization is thus the process of identifying meaningful tokens, however "meaningful" is defined for a particular application in a particular language, and may cover items traditionally identified as "words" as well as clitics, punctuation, and other errata. Note, however, that differences in conventions regarding what constitutes a meaningful token at a given level of analysis still have to be taken into account during evaluation.

While his algorithm requires no prior knowledge of linguistic structure, Aken\cite{aken11} does note that "A shortcut [...] is to incorporate a ready-made dictionary that contains some or all of the words that the algorithm will encounter." This lexical-access approach is most clearly demonstrated by Norvig\cite{norvig14}, who describes the of a simple lexicon to recursively split a string into a known word and a suffix of remaining characters, using dynamic programming techniques to construct a lattice to avoid the inefficiency of recomputing overlapping segmentations for the same suffixes multiple times. Word-level n-gram frequency data is then used to extract the highest probability tokenization from the lattice. Norvig\cite{norvig14} cites applications to Chinese, as well as to specialized genres of English such as URLs which are written without spaces.
Norvig's algorithm, however, is only as good as the dictionary- it will fail on encountering out-of-vocabulary (OOV) words. This makes it less suitable for application to languages that have extensive productive morphology, such as Turkish, in which it may be literally impossible to encode the complete (infinite) set of possible words in a dictionary, even before we account for the fact that new words appear constantly in living languages\cite{islam07}. Although this overlaps somewhat with the weaknesses of Aken's algorithm, the potential benefits of a hybrid approach are already evident, as a strong statistical algorithm may be able to make up for gaps in vocabulary.

\section{Orthographies Traditionally Lacking Whitespace}
Most practical work in text tokenization has been done in languages whose writing systems traditionally have no spaces or other written word breaks, such as Chinese, Japanese, or Thai. In these cases, much like the special rules for Treebank annotation, separate tokenization systems are typically developed on a per-language basis, often using wildly different approaches \cite{peng04} \cite{suzuki00}.

Peng, Feng, \& McCallum\cite{peng04} developed a system for Chinese segmentation which recasts segmentation as a tagging problem using linear-chain conditional random fields (CRFs); every character is tagged as either a start-of-word character, or not. This is an inherently hybrid system, as the CRF models incorporate lexical knowledge in addition to lower-level statistics, which are used for probabilistic new-word detection. This provides the best of both worlds: you have the efficiency and accuracy improvements that come with lexical access, but avoid the need to store the entire vocabulary and allow for the proper identification of unknown words as long as they do not exhibit structural characteristics too far removed from those used to create the statistical model- i.e., the normal morphographological\footnote{The written parallel to morphophonological.} structure of the language.

The state-of-the-art Japanese segmentation system by Kudo, Yamamoto, \& Matsumoto\cite{kudo04} also makes use of CRFs to simultaneously achieve tokenization and morphological analysis by selecting the best path through a lattice of possible \textit{morpheme}-level tokenizations generated from a morphologically-aware dictionary. In fact, Kudo et al. were primarily motivated by the problem of morphological analysis of unlabelled text, for which automatic tokenization was merely an unavoidable first step when working with Japanese orthography. This builds on prior work by Asahara et al.\cite{asahara00} and Uchimoto et al.\cite{uchimoto01} on Japanese tokenization and morphanalysis using HMMs and Maximum Entropy Markov Models (MEMMs), respectively. Kudo et al.'s CRF model was found to improve on both HMM and MEMM models due to the ability of CRFs to make use of a wider range of tagging features, including lexical information, providing additional evidence for the utility of hybrid segmentation approaches.

Choosing to work at the level of morphemes rather than words additionally removes many of the problems present in other systems with hapax legomena or OOV words. This requires, however, either the ability to re-synthesize complete words from morphemes, or an agreement with later stages of the NLP pipeline to define "tokens" at the appropriate sub-word level. Where it possible, however, the ability to achieve simultaneous tokenization and morphanalysis is a huge win in terms of improving efficiency and being able to provide more contextual information to later stages of an NLP pipeline, such as syntactic parsing.

Suzuki, Brockett, and Kacmarcik\cite{suzuki00}, in fact, took this one step further, by using a syntactic model to simultaneously select an optimal tokenization while producing a syntactic parse. Like Kudo et al.\cite{kudo04}, Asahara et al.\cite{asahara00} and Uchimoto et al.\cite{uchimoto01}, they also use a "word breaker" that performs simultaneous morphanalysis while producing a lattice of possible tokenizations, but simplify it by removing the responsibility for collapsing the information in the lattice into a single unambiguous best token sequence. Instead, the lattice is used as the bottom level of a a parsing chart, with the selection of the correct token sequence being a natural consequence of identifying the best syntax tree.

% I thought about adding references to "Incremental Joint Approach to Word Segmentation, POS
% Tagging, and Dependency Parsing in Chinese", but it doesn't really cover anything new that's
% missing from any of the existing references, and I don't want to focus too much on Chinese
% when I'm not actually doing anything with Chinese.

In my own previous work\cite{kearsley13}, I addressed the problem of efficient lattice generation with simultaneous morphanalysis using a finite-state transducer based on the KIMMO two-level morphology system \cite{koskenniemi84}. This system is capable of generating a lattice of all possible sequences of known morphemes that could be extracted from an input stream in amortized constant time per-character performance, or amortized linear time performance over arbitrary sized inputs, as each input character needs to be examined exactly once and in document order in order to determine which morphemes it could be part of\cite{kearsley13}. Storing the lexicon as a finite-state transducer both saves a great deal of space and also allows recognition of a potentially infinite number of regularly derived or inflected words, though with some reduction in flexibility compared to the CRF models. Compared to a fixed lexicon as used by Norvig\cite{norvig14}, a morphological transducer suffers less from from the problem of encountering OOV tokens, but does not solve the problem completely- it will still be tripped up by unknown roots\footnote{And potentially other kinds of unknown morphemes, though I assume that, e.g., unknown derivational affixes are much more unlikely than unknown members of open-class lexical roots.}
The great advantage of this approach is that nearly any morphological model can be adapted to use in a simultaneous lexical-access based tokenizer (although it is optimized for FST-based models); in other words, the language model does not need to be developed with tokenization specifically in mind, and existing analyzers can be re-used, potentially saving a great deal of effort that would otherwise go into another bespoke, language-specific tokenization system. Additionally, this approach is specifically designed for on-line, real-time usage, and, while that feature is common (nearly obligatory, in fact) for speech processing, it is relatively rare in text tokenization\cite{aken11}. Like Suzuki et al.\cite{suzuki00}, however, I clearly separated the step of lattice generation from optimal path selection, and focused only on the former- an approach which I continue in this thesis.

%I feel like this might be better in the Methodology chapter, as it feels a little weird to detour away from tokenization and talk about _just_ morphanalysis.

%NOTE: the efficiency advantages are realized by any network-based system, which includes HMMS, MEMMS, and decision trees.


\section{Psychological Modeling}

In addition to the NLP-oriented approach, the tokenization problem has also been studied from the point of view of language acquisition (most typically, L1 acquisition): When hearing a new language that one does not know, how does a learner begin to identify new words to add to their mental lexicon? This clearly requires some generic mechanism (although of unknown and possibly great computational complexity) for inducing probable words boundaries (where words can then be assumed to be contiguous sequences between two boundaries) from unlabeled input. That humans are able to do this provides an existence proof of the technical possibility of language-agnostic tokenization.

While studying the ways that human minds may tackle tokenization is not guaranteed to produce the best possible artificial tokenization system since they may involve unnecessary complexity or require quantities of data infeasible to obtain for many NLP applications (in the same way that the best way to build an airplane is not to exactly copy a flapping bird's wing), studies of human psychological segmentation mechanisms do serve two useful purposes: They set a baseline against which errors can be measured, and they may provide an initial example implementation from which further engineering can draw\cite{daland09}.

To establish a baseline of performance, it is sufficient to use manually-tagged corpora as a gold standard to test against; to provide an example implementation for re-engineering, it is important to have a reasonable psychological model of human speech segmentation. Daland\cite{daland09} argues that both statistical predictive mechanisms and lexical access are necessarily involved in human segmentation, with contributions from each shifting over the language acquisition process: statistical mechanisms in order to guess at the boundaries around new unknown words and to determine the probability that a potential bounded sequence is in fact a new word, and lexical access to filter errors. Empirical evidence for the validity of this claim is provided by Islam \cite{islam07}, who reports that many state-of-the-art NLP systems do in fact use hybrid approaches. Daland\cite{daland09} further proposes that there are only two feasible error-correction conditions for the human tokenization system: statistical over-recognition, in which case lexical access must filter out erroneously predicted word boundaries; and statistical under-recognition, in which case lexical access must be prepared to add statistically unlikely word boundaries; logically, however, both may occur in any newly-developed system.

In either case, we can expect an improvement from merging statistical and lexicon-based methods as the set of correct word boundaries found by either method would not a priori be expected to form a subset of those found by the other; thus, their unions would be a larger set of correct boundaries. However, in the case of statistical under-recognition, we would expect a smaller contribution from statistical boundary prediction to the recognition of unknown words by lexicon-based methods. We would thus expect that statistical over-recognition would be the norm, and indeed, this is exactly the result obtained by Rytting\cite{rytting04}.

\section{Simplified Statistical Models}

Revisiting the analogy of the airplane vs. bird wing, one is led to wonder if there might be statistical models much simpler than CRFs or HMMs that are still in some sense "good enough".  Brent, motivated, like Daland, by the study of first language acquisition, identified distribution regularities at the phoneme (or grapheme) level, such as segment-to-segment transition probabilities, to be useful indicators of word boundaries\cite{brent96}. In later work, Brent\cite{brent99} and Rytting\cite{rytting04} both investigated simple character-level statistical cues that could be used as effective predictors of word boundaries during language acquisition, and for tokenization of text. The identification of such useful "minimal information" predictors holds significant promise for developing simplifications of Daland's cognitive model\cite{daland09} and further improving the performance of hybrid tokenizers.
While Brent\cite{brent99} assumed that a prediction technique based on local comparisons (i.e., looking for significant differences between the values of a given statistical measurement between adjacent positions in the text), would be \textit{most} appropriate for identifying word boundaries, Rytting\cite{rytting04} argued that this is guaranteed to under-predict due to the impossibility of correctly identifying common single-segment words, since the values of any two measurements in immediately adjacent positions cannot simultaneously be either significantly greater than or significantly less than each other. This echoes the problems with identification of small words encountered by Aken\cite{aken11} with a significantly more complex model. Global comparisons are therefore needed to ensure maximum recall of correct boundaries.
Rytting\cite{rytting04} tested the predictive power of two statistics based on Brent's work\cite{brent99}: character-level transition probabilities, and character bigram mutual information scores, using both local and global comparisons for each. Rytting \cite{rytting04} showed that, on Greek text, a very simple mutual-information (MI) model was both better than the competing transition-probability models and highly over-predictive on its own, and indeed that using a global threshold for comparison was the most effective of the techniques that he tested. This is despite the fact that the threshold value of zero used in Rytting's study\cite{rytting04} was chosen "arbitrarily" with no justification attempted. Rytting's model, however, does also take into account the probability, based on a pre-annotated training corpus, that any particular bigram represents a known word boundary.

In my own previous work\cite{kearsley14}, I applied the MI approach to English, testing the sensitivity of the model to the quantity of training data available for calculating bigram MI scores. In contrast to Rytting's supervised approach, however, I employed a further simplified, completely unsupervised algorithm which relied solely on MI scores calculated from an unanotated corpus of natural English text. Additionally, in response to Rytting's claim that the zero threshold is "arbitrary", I compared its performance with an alternative "gap threshold" assignment method. In this approach, a model-specific threshold is dynamically selected by dividing bigrams into two clusters based on the largest gap between successive MI scores.

While the results are not directly comparable since our experiments were done on different languages, the unsupervised zero threshold MI approach in fact produced better accuracies than Rytting's supervised approach\cite{rytting04}, although it comes at a cost in recall. The gap threshold approach, on the other hand, showed high recall, but lower accuracy. I also found that, above a certain relatively small minimum size, results were not strongly dependent on the size of the training corpus. This, combined with the demonstrated efficacy of the unsupervised
approach, means that the data required to build useful statistical word boundary prediction
systems can be acquired cheaply, without the need for human annotation. Neither my nor Rytting's results, however, show recall scores close to 100\%, which, together with the early performance plateau, supports Daland's thesis that human tokenization must transition from primarily static-based to more lexical-access based methods during the learning process. While these extremely simple systems based on a single statistical measure (in this case, mutual information) do not have sufficiently good performance to use entirely on their own, their improvement over a
random baseline is sufficient to justify their use as heuristic filters to improve the performance of more complex hybrid word segmentation systems\cite{kearsley14}.

\section{Path Selection Methods}
While my work sidesteps the problem of path selection, it is worth briefly reviewing what options are available. Suzuki et al.'s\cite{suzuki00} parser-based approach, while highly effective, is also very heavyweight, and several simpler options exist for modular systems that separate lattice generation from path selection. Kudo\cite{kudo04}, as justification for the need to incorporate lexical knowledge, cites accuracy of over 90\% using a simple longest-prefix matching algorithm, which simply assumes that the longest prefix of a string which is a known token is a correct token, and then restarts after that longest prefix to find the next token. It is unclear, however, whether the cited statistic applies only to Japanese, or cross-linguistically. Islam\cite{islam07} also describes a "maximal match" algorithm (with several variations for resolving ambiguities), which essentially consists of finding the longest substring that is a known token (regardless of whether or not it is a prefix of the text), and then proceeding to select the next longest known tokens in the remaining non-overlapping sections of text. Their particular modification, using type frequency and entropy measurements to resolve ambiguity between equal-length options, achieves over 90\% in both precision and recall on the Brown corpus. This method is particularly useful in restricted settings where it is necessary to identify correct instances of certain types in desegmented text, but where complete tokenization and identification of unknown tokens is unnecessary; for example, a "maximal match" algorithm is used in the TIARA\footnote{The Interactive Annotated Reader Application} and Ayamel software developed by BYU's ARCLITE Lab in order to identify words and phrases that require annotations to be applied- an application in which normal English tokenization conventions are completely unsuitable, as the relevant tokens are often large than single words and may contain spaces.
%I know of other instances where this is used for essentially the same purpose, but I'm not certain if I'm allowed to cite them or not due to NDAs
Norvig\cite{norvig14} provides an approach which incorporates language-specific knowledge but is still relatively simple, calculating the most likely lattice traversal using n-gram probabilities, as noted above. Accurate results from this approach, however, depend on access to an extremely large training corpus (over 3 billion words in this case) to ensure that accurate statistics are available for all possible n-grams; the algorithm will fail when presented not merely with OOV tokens, but with previously unseen sequences of known tokens. This problem can be significantly ameliorated by replacing the simple word n-gram model with a part-of-speech (POS) tag model, which can produce accurate statistics with a much smaller training corpus, since one tag sequence equates to several concrete word sequences. The tag-based approach is particularly attractive for use with systems that employ simultaneous morphanalysis, since POS information comes essentially "for free".

\section{Lattice Generation with Morphology \& Statistics}

Regardless of the path selection algorithm used, the final results are only as good as the lattice that is fed into them. If a lattice generation step cannot produce all of the correct token boundaries as hypotheses, then no selection algorithm can produce the complete correct token sequence. On the other hand, a naive conservative lattice generation algorithm could simply propose \textit{every} character boundary as a possible token boundary, in which case the correct sequence is guaranteed to exist somewhere, and could be found- but only in a highly inefficient manner, as the lattice generator has provided no useful information to guide the search. A separate lattice generation algorithm can thus be rated both on how many correct hypotheses its output contains, and on how much work it requires of the succeeding path selection algorithm. In this context, lexicon-based tokenizers have an additional advantage over many pure statistical algorithms in that they inherently produce pairs of starting and ending boundaries for specific tokens; this significantly reduces redundancy compared to single-boundary tagging systems\cite{kudo04} like those of Peng et al.,\cite{peng04}, Brent\cite{brent99}, or Rytting\cite{rytting04}, which in the worst case can require considering a number of token hypotheses proportional to the square of the number of individual boundary hypotheses, significantly slowing down the tokenization process.

Building on my previous work, I have built a series of hybrid lattice-generation systems based on fundamentally language-agnostic lexical-access based and simple statistical algorithms, employing pre-existing morphological analyzers to provide language-specific lexical knowledge. It is expected that these relatively simple models will produce good results, improving on the performance of either lexical-access based or simple statistical methods alone, across multiple languages that are orthographically and typologically (specifically, morphologically) dissimilar.