\chapter{Conclusions \& Future Work}

While precision scores are highly sensitive to the details of a specific language's morphology and orthography, morphological tokenization generalizes well across the three languages I tested in terms of recall. In each case, recall scores are limited by the quality of the morphological analyzer used. Additionally, a simplified statistical model using character bigram mutual information scores with a zero-threshold was effective at identifying unknown tokens like numbers and punctuation marks, which are likely to be left out of typical morphological analyses lexicons. This significantly improves recall performance in a hybrid tokenizer across multiple languages as well, even though the statistical approaches do not produce practical results in isolation.

While the simplified statistical models employed for these experiments did prove useful for improving the performance of a hybrid tokenizer in a limited domain, more complex models are needed to precisely identify unknown words whose boundary statistics are too similar to other words for MI scores to reliably distinguish. Additional research is also warranted on higher-recall statistical models that maintain reasonable precision to improve filtering efficiency. In principle, separate statistical models tuned for the different use cases could be employed for simultaneous filtering and filling, with an effective filter providing more opportunity for filling model to act by removing spurious morphological analyses and thus correctly identifying more spans as unknown.

While I achieved good results on Arabic, English, and Korean, these three languages do not cover the whole spectrum of variation seen in scripts for natural languages. A better picture of the generalizability\textemdash~or limits thereof\textemdash~of these algorithms could be obtained by testing on additional languages like Turkish (which is highly agglutinative and alphabetic), Thai (which uses an abugida with no spaces), Japanese, and Chinese.
Japanese is particularly interesting for the use of multiple scripts simultaneously, with transitions between kanji and hiragana providing significant token-boundary clues which may be easily picked up by a statistical model. Similarly, it would be useful to test on English corpora which include tokens that contain spaces, such as open compounds, borrowings, and idiomatic phrases; ENGLEX has some of these in its lexicon, but the answer keys derived from OANC annotations do not recognize them, and being able to evaluate the morphological tokenizer against an answer key whose tokenization conventions more closely match those of the analyzer's lexicon would improve the case for using this method over simple whitespace splitting even in languages which do use whitespace for the majority of token boundaries, as English does.

Additional small improvements to recall could be made by addressing the problem of overlapping tokens. This is most easily demonstrated by ambiguity in English periods\textemdash~in a sentence like ``The Peloponnesian War concluded in 404 B.C.", the final period is both a part of the token ``B.C.", and an independent token marking the end of the sentence\textemdash~two tokens overlap by a single character. Various forms of non-concatenative morphology or sandhi can create other cases where it is clear that there are multiple tokens, but not clear where precisely one token ends and another begins. This is difficult to handle when tokens are tied to specific positions in the input stream, but would be sidestepped if the data structure used to represent a lattice could preserve the logical ordering of tokens independent of their positions in the input stream.

Of course, having identified a cross-linguistically useful lattice generation algorithm, the next step in the pipeline is identifying a cross-linguistically useful path selection algorithm to match with it. Building on the work of Suzuki et al in Japanese \cite{suzuki00}, it should be possible to design a generic on-line probabilistic parser which would then only require input of a language-specific syntactic model to do simultaneous tokenization, morphanalysis, part-of-speech tagging, and parsing, employing both bottom-up and top-down information at every stage. This would be more complicated, but still doable, given a lattice in which tokens are topologically sorted, but missing a consistent connection to input indices. Given the relative complexity of developing usefully-complete syntactic models, however, there is of course room for exploring the cross-linguistic applicability of shallower statistical models, like the smoothed token n-grams used by Norvig \cite{norvig14} or POS-tagging models (which, like syntactic models, would also assist in disambiguation of tokens with multiple morphological analyses). A slightly more exotic approach would be determining most-likely paths based on word association models like those produced by word2vec \cite{mikolov13}. This would pair particularly well with the morphological algorithm's ability to identify single logical tokens that happen to contain traditional boundary markers in a language's standard orthography, such as idiomatic phrases and open compounds containing spaces, since Mikolov et al's algorithm explicitly attempts to identify idiomatic phrases through simple statistical methods during training, even when they are not identified as single tokens in the training corpus (as is the case in the OANC), thus reducing the possibility of errors due to mismatches in the effective tokenization conventions used by the lattice generator and path selection algorithm.