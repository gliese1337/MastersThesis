Tokenization, or word boundary detection, is a critical first step for most NLP applications. This is often glossed over in English and other languages which use explicit spaces between written words, but standard orthographies for many languages lack explicit markers. Tokenization systems for such languages are usually engineered on an individual basis, with little re-use. The human ability to decode any written language, however, suggests that a general algorithm exists.

This paper presents simple morphologically-based and statistical methods for identifying word boundaries in multiple languages. Statistical methods tend to over-predict, while lexical and morphological methods fail when encountering unknown words. I demonstrate that a morphological approach to tokenization generalizes well across multiple languages, and show how a simple hybrid approach can improve performance and be used for efficient tokenization of English, Korean, and Arabic.