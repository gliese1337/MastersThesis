Dictionary-based tokenization approaches are common in languages without written word boundaries such as Japanese and Chinese, but can be useful even in languages like English given that spaces and punctuation are not perfectly accurate indicators of word or clause boundaries. Dictionary-based tokenizers, however, can suffer from the need to store all possible words and the inability to recognize regular morphological forms that do not occur in the training data. Using a morphological analyzer in place of a static dictionary ameliorates these issues, but does not entirely eliminate the unknown-word problem. I show how a hybrid approach using a morphological analyzer and statistical tokenization methods can be used for efficient tokenization of English, Korean, and Arabic. 